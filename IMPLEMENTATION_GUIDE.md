# ğŸ”§ AI ë¡œì§ êµ¬í˜„ ê°€ì´ë“œ

> `generation/chatbot/chatbot.py` êµ¬í˜„ì„ ìœ„í•œ ìƒì„¸ ê°€ì´ë“œ

---

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” `chatbot.py`ì˜ 10ê°œ TODOë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„  
**ë‚œì´ë„**: ì¤‘ê¸‰  
**ì „ì œ ì§€ì‹**: Python ê¸°ì´ˆ, API ì‚¬ìš© ê²½í—˜

---

## ğŸ¯ ì „ì²´ êµ¬ì¡° ì´í•´í•˜ê¸°

### ì±—ë´‡ ë™ì‘ íë¦„

```
1. ì‚¬ìš©ì ë©”ì‹œì§€ ì…ë ¥
   â†“
2. RAG ê²€ìƒ‰ (ChromaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°)
   â†“
3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìºë¦­í„° ì„¤ì • + RAG ì»¨í…ìŠ¤íŠ¸)
   â†“
4. LLM ì‘ë‹µ ìƒì„± (OpenAI API í˜¸ì¶œ)
   â†“
5. ë©”ëª¨ë¦¬ ì €ì¥ (ëŒ€í™” ê¸°ë¡ ìœ ì§€)
   â†“
6. ì‘ë‹µ ë°˜í™˜
```

### í•µì‹¬ ê°œë…

#### 1. RAG (Retrieval-Augmented Generation)
```
Q: "í•™ì‹ ì¶”ì²œí•´ì¤˜"
   â†“
RAG ê²€ìƒ‰: "í•™ì‹ì€ ê³¤ìê°€ê°€ ë§›ìˆì–´" (ì‚¬ì „ ì €ì¥ëœ ì •ë³´)
   â†“
LLM: "í•™ì‹ì€ ê³¤ìê°€ì—ì„œ ë¨¹ëŠ” ê²Œ ì¢‹ì•„. ëˆê¹ŒìŠ¤ê°€ íŠ¹íˆ ì¸ê¸°ì•¼!"
```

#### 2. ì„ë² ë”© (Embedding)
```
í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜
"í•™ì‹ ì¶”ì²œ" â†’ [0.1, 0.3, -0.2, ..., 0.5]  (3072ì°¨ì›)

ìœ ì‚¬ë„ ê³„ì‚°:
"í•™ì‹ ì¶”ì²œ" [0.1, 0.3, ...] 
    vs
"í•™ì‹ì€ ê³¤ìê°€" [0.12, 0.28, ...]
    â†’ ìœ ì‚¬ë„: 0.89 (ë†’ìŒ!)
```

#### 3. ChromaDB
```
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- ì„ë² ë”© ì €ì¥
- ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰
- ë©”íƒ€ë°ì´í„° ê´€ë¦¬
```

---

## ğŸ“ TODO 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# OpenAI API
from openai import OpenAI

# ChromaDB (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)
import chromadb

# LangChain (LLM í†µí•© í”„ë ˆì„ì›Œí¬)
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# í•œêµ­ì–´ ì²˜ë¦¬
from konlpy.tag import Okt

# ìœ í‹¸ë¦¬í‹°
import uuid  # ê³ ìœ  ID ìƒì„±
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
```

### ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì—­í• 

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì—­í•  | ì‚¬ìš© ì´ìœ  |
|---------|------|---------|
| `openai` | OpenAI API í˜¸ì¶œ | GPT, ì„ë² ë”© ìƒì„± |
| `chromadb` | ë²¡í„° DB | ì„ë² ë”© ì €ì¥/ê²€ìƒ‰ (RAG) |
| `langchain` | LLM í†µí•© | ë©”ëª¨ë¦¬, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ |
| `konlpy` | í•œêµ­ì–´ NLP | í‚¤ì›Œë“œ ì¶”ì¶œ |

---

## ğŸ“ TODO 2: ì„¤ì • íŒŒì¼ ë¡œë“œ

### êµ¬í˜„ ì½”ë“œ

```python
CONFIG_PATH = BASE_DIR / 'config' / 'chatbot_config.json'

try:
    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        config = json.load(f)
except FileNotFoundError:
    print(f"Warning: config file not found at {CONFIG_PATH}")
    config = {}
```

### ì„¤ì • íŒŒì¼ êµ¬ì¡°

```json
{
  "name": "ì±—ë´‡ ì´ë¦„",
  "character": {
    "age": 20,
    "university": "ëŒ€í•™êµ",
    "major": "ì „ê³µ"
  },
  "system_prompt": {
    "base": "ë‹¹ì‹ ì€...",
    "rules": ["ë°˜ë§ ì‚¬ìš©", "..."]
  }
}
```

---

## ğŸ“ TODO 3: OpenAI API í‚¤ ì„¤ì •

### êµ¬í˜„ ì½”ë“œ

```python
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError(
        "OPENAI_API_KEY not found in environment variables. "
        "Please set it in .env file"
    )

client = OpenAI(api_key=api_key)
```

### í…ŒìŠ¤íŠ¸

```python
# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
try:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "ì•ˆë…•"}],
        max_tokens=10
    )
    print("API ì—°ê²° ì„±ê³µ:", response.choices[0].message.content)
except Exception as e:
    print("API ì˜¤ë¥˜:", e)
```

---

## ğŸ“ TODO 4: ChromaDB ì´ˆê¸°í™”

### êµ¬í˜„ ì½”ë“œ

```python
def init_text_db():
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    
    # 1. ì €ì¥ ê²½ë¡œ ì„¤ì •
    db_path = BASE_DIR / "static" / "data" / "chatbot" / "chardb_embedding"
    
    # 2. ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´)
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    # 3. ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    dbclient = chromadb.PersistentClient(path=str(db_path))
    
    # 4. ì»¬ë ‰ì…˜ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê¸°)
    collection = dbclient.get_or_create_collection(
        name="rag_collection"
    )
    
    print(f"[DB] ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
    print(f"[DB] ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {collection.count()}")
    
    return dbclient, collection


# ì „ì—­ ë³€ìˆ˜ë¡œ ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
text_dbclient, collection = init_text_db()
```

### ChromaDB êµ¬ì¡°

```
chardb_embedding/
â”œâ”€â”€ chroma.sqlite3        # ë©”íƒ€ë°ì´í„° ì €ì¥
â””â”€â”€ [UUID]/               # ì‹¤ì œ ë²¡í„° ë°ì´í„°
    â”œâ”€â”€ data_level0.bin
    â””â”€â”€ ...
```

---

## ğŸ“ TODO 5: ì„ë² ë”© ìƒì„± í•¨ìˆ˜

### êµ¬í˜„ ì½”ë“œ

```python
def get_embedding(text, model="text-embedding-3-large"):
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    
    try:
        # OpenAI API í˜¸ì¶œ
        response = client.embeddings.create(
            input=[text],
            model=model
        )
        
        # ì„ë² ë”© ë²¡í„° ë°˜í™˜ (3072ì°¨ì›)
        embedding = response.data[0].embedding
        
        # ë¡œê·¸ ì¶œë ¥ (ì„ íƒ)
        print(f"[EMBED] í…ìŠ¤íŠ¸: {text[:50]}...")
        print(f"[EMBED] ë²¡í„° ì°¨ì›: {len(embedding)}")
        
        return embedding
        
    except Exception as e:
        print(f"[EMBED] ì˜¤ë¥˜: {e}")
        raise
```

### ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | ì°¨ì› | ì„±ëŠ¥ | ë¹„ìš© |
|------|-----|------|-----|
| text-embedding-3-small | 1536 | ë³´í†µ | ì €ë ´ |
| text-embedding-3-large | 3072 | ë†’ìŒ | ë¹„ìŒˆ (ê¶Œì¥) |

### í…ŒìŠ¤íŠ¸

```python
# ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
text = "ì•ˆë…•í•˜ì„¸ìš”"
embedding = get_embedding(text)
print(f"ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 5ê°œ): {embedding[:5]}")
# ì¶œë ¥: [0.123, -0.456, 0.789, ...]
```

---

## ğŸ“ TODO 6: í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„ íƒ)

### êµ¬í˜„ ì½”ë“œ

```python
okt = Okt()

def extract_nouns_korean(text):
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    # 1. ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)
    
    # 2. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´)
    stopwords = [
        'ê²ƒ', 'ìˆ˜', 'ë“±', 'ì•ˆ', 'ë•Œ', 'ê³³', 'ì¤‘', 'ë“¤', 
        'ê±°', 'ì œ', 'ê°„', 'ë‚´', 'ë‚˜', 'ì ', 'ê²Œ'
    ]
    
    # 3. í•„í„°ë§ (ë¶ˆìš©ì–´ ì œê±°, 2ê¸€ì ì´ìƒë§Œ)
    keywords = [
        noun for noun in nouns 
        if noun not in stopwords and len(noun) > 1
    ]
    
    print(f"[KEYWORD] ì›ë¬¸: {text}")
    print(f"[KEYWORD] ì¶”ì¶œ: {keywords}")
    
    return keywords
```

### ì˜ˆì‹œ

```python
text = "í•™ì‹ ì¶”ì²œí•´ì¤˜. ì–´ë””ê°€ ë§›ìˆì–´?"
keywords = extract_nouns_korean(text)
# ì¶œë ¥: ['í•™ì‹', 'ì¶”ì²œ', 'ë§›']
```

### ì™œ í•„ìš”í•œê°€?

```
ì¼ë°˜ ê²€ìƒ‰:
"í•™ì‹ ì¶”ì²œí•´ì¤˜" â†’ ì„ë² ë”© â†’ ê²€ìƒ‰ (ìœ ì‚¬ë„: 0.65)

í‚¤ì›Œë“œ ì¶”ê°€:
"í•™ì‹ ì¶”ì²œí•´ì¤˜ + í•™ì‹ ì¶”ì²œ" â†’ ì„ë² ë”© â†’ ê²€ìƒ‰ (ìœ ì‚¬ë„: 0.85 â†‘)
```

---

## ğŸ“ TODO 7: RAG ë¬¸ì„œ ê²€ìƒ‰ (í•µì‹¬!)

### êµ¬í˜„ ì½”ë“œ

```python
def search_similar_documents(query, collection, threshold=0.45, top_k=5):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (RAGì˜ í•µì‹¬)"""
    
    try:
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ (ì„ íƒ, ì •í™•ë„ í–¥ìƒ)
        keywords = extract_nouns_korean(query)
        keyword_text = " ".join(keywords)
        combined = f"{query} {keyword_text}"
        
        print(f"\n[RAG] ì§ˆë¬¸: {query}")
        print(f"[RAG] í‚¤ì›Œë“œ: {keywords}")
        
        # 2. ì„ë² ë”© ìƒì„±
        query_embedding = get_embedding(combined)
        
        # 3. ChromaDBì—ì„œ ê²€ìƒ‰
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "distances", "metadatas"]
        )
        
        # 4. ê²°ê³¼ í™•ì¸
        if not results["documents"] or not results["documents"][0]:
            print("[RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return None, None, None
        
        documents = results["documents"][0]
        distances = results["distances"][0]
        metadatas = results["metadatas"][0]
        
        # 5. ìœ ì‚¬ë„ ê³„ì‚° ë° í•„í„°ë§
        filtered_docs = []
        
        for doc, dist, meta in zip(documents, distances, metadatas):
            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            similarity = 1 / (1 + dist)
            
            if similarity >= threshold:
                filtered_docs.append((doc, similarity, meta))
                print(f"[RAG] ë°œê²¬: {doc[:50]}... (ìœ ì‚¬ë„: {similarity:.4f})")
        
        # 6. ìµœê³  ìœ ì‚¬ë„ ë¬¸ì„œ ë°˜í™˜
        if filtered_docs:
            best_doc = max(filtered_docs, key=lambda x: x[1])
            print(f"[RAG] ì„ íƒ: ìœ ì‚¬ë„ {best_doc[1]:.4f}")
            return best_doc  # (document, similarity, metadata)
        
        print(f"[RAG] threshold({threshold}) ì´ìƒ ê²°ê³¼ ì—†ìŒ")
        return None, None, None
        
    except Exception as e:
        print(f"[RAG] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None, None, None
```

### threshold ê°’ ì„ íƒ

| threshold | íš¨ê³¼ | ì‚¬ìš© ì‹œê¸° |
|-----------|------|---------|
| 0.3-0.35 | ë§ì€ ë¬¸ì„œ ê²€ìƒ‰ | ë°ì´í„°ê°€ ì ì„ ë•Œ |
| 0.40-0.45 | ê· í˜• (ê¶Œì¥) | ì¼ë°˜ì ì¸ ê²½ìš° |
| 0.50-0.60 | ì—„ê²©í•œ ê²€ìƒ‰ | ë°ì´í„°ê°€ ë§ì„ ë•Œ |

### ìœ ì‚¬ë„ ê³„ì‚° ì´í•´

```
ê±°ë¦¬ (distance): ë²¡í„° ê°„ ê±°ë¦¬ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
  ì˜ˆ: distance = 0.5

ìœ ì‚¬ë„ (similarity): 1 / (1 + distance)
  ì˜ˆ: similarity = 1 / (1 + 0.5) = 0.67

threshold = 0.45ì¼ ë•Œ:
  0.67 >= 0.45 â†’ âœ… í†µê³¼
```

---

## ğŸ“ TODO 8: LangChain ë©”ëª¨ë¦¬ ë° ëŒ€í™” ì²´ì¸

### êµ¬í˜„ ì½”ë“œ

```python
# 1. LLM ì´ˆê¸°í™”
langchain_llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    openai_api_key=api_key,
    temperature=0.7  # 0.3(ì •í™•) ~ 1.0(ì°½ì˜ì )
)

# 2. ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ConversationSummaryBufferMemory(
    llm=langchain_llm,
    memory_key="chat_history",
    input_key="input",
    max_token_limit=1000,  # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
    return_messages=True
)

# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """
{system_prompt}

{chat_history}

ì‚¬ìš©ì: {input}

ì±—ë´‡:
"""

prompt = PromptTemplate(
    input_variables=["system_prompt", "input", "chat_history"],
    template=template
)

# 4. ëŒ€í™” ì²´ì¸ ìƒì„±
conversation_chain = LLMChain(
    llm=langchain_llm,
    prompt=prompt,
    memory=memory,
    verbose=False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
)
```

### ë©”ëª¨ë¦¬ ë™ì‘ ë°©ì‹

```
1ì°¨ ëŒ€í™”:
  ì‚¬ìš©ì: "ì•ˆë…•?"
  ì±—ë´‡: "ì•ˆë…•! ë­ ë„ì™€ì¤„ê¹Œ?"
  ë©”ëª¨ë¦¬: [ì‚¬ìš©ì: ì•ˆë…•?, ì±—ë´‡: ì•ˆë…•! ë­ ë„ì™€ì¤„ê¹Œ?]

2ì°¨ ëŒ€í™”:
  ì‚¬ìš©ì: "ì•„ê¹Œ ë­ë¼ê³  í–ˆì–´?"
  ë©”ëª¨ë¦¬ ì°¸ì¡°: "ì•ˆë…•! ë­ ë„ì™€ì¤„ê¹Œ?"
  ì±—ë´‡: "ì•„ê¹Œ 'ì•ˆë…•! ë­ ë„ì™€ì¤„ê¹Œ?'ë¼ê³  í–ˆì–´"
```

---

## ğŸ“ TODO 9: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

### êµ¬í˜„ ì½”ë“œ

```python
def build_system_prompt(username, has_context=False):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    # 1. configì—ì„œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    base_prompt = config.get('system_prompt', {}).get(
        'base', 
        'ë‹¹ì‹ ì€ ì¹œê·¼í•œ ì±—ë´‡ì…ë‹ˆë‹¤.'
    )
    
    rules = config.get('system_prompt', {}).get('rules', [])
    rules_text = '\n'.join(f"- {rule}" for rule in rules)
    
    # 2. ìºë¦­í„° ì •ë³´ êµ¬ì„±
    character = config.get('character', {})
    char_info = f"""
ìºë¦­í„° ì •ë³´:
- ë‚˜ì´: {character.get('age', 'ì•Œ ìˆ˜ ì—†ìŒ')}
- ëŒ€í•™: {character.get('university', 'ì•Œ ìˆ˜ ì—†ìŒ')}
- ì „ê³µ: {character.get('major', 'ì•Œ ìˆ˜ ì—†ìŒ')}
- ì„±ê²©: {character.get('personality', 'ì•Œ ìˆ˜ ì—†ìŒ')}
- ë°°ê²½: {character.get('background', 'ì•Œ ìˆ˜ ì—†ìŒ')}
"""
    
    # 3. RAG ì»¨í…ìŠ¤íŠ¸ ì§€ì‹œì‚¬í•­
    context_instruction = ""
    if has_context:
        context_instruction = """
[ì¤‘ìš”] ëŒ€í™” ì¤‘ì— [ì°¸ê³  ì •ë³´]ê°€ ì œê³µë˜ë©´, 
ì´ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ë‹¨, ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ë§í•˜ê³ , 
"ì°¸ê³  ì •ë³´ì— ë”°ë¥´ë©´..."ì²˜ëŸ¼ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
"""
    
    # 4. ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
    system_prompt = f"""
{base_prompt}

{char_info}

ëŒ€í™” ê·œì¹™:
{rules_text}

{context_instruction}

[ì‚¬ìš©ì ì´ë¦„: {username}]
"""
    
    return system_prompt
```

### í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

```
ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ 4í•™ë…„ ê¹€ì„œê°•ì…ë‹ˆë‹¤.

ìºë¦­í„° ì •ë³´:
- ë‚˜ì´: 24ì„¸
- ëŒ€í•™: ì„œê°•ëŒ€í•™êµ
- ì „ê³µ: ì»´í“¨í„°ê³µí•™ê³¼
- ì„±ê²©: ì¹œì ˆí•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•¨
- ë°°ê²½: ì‹ ì…ìƒì„ ë•ëŠ” ì„ ë°°

ëŒ€í™” ê·œì¹™:
- ë°˜ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”

[ì¤‘ìš”] [ì°¸ê³  ì •ë³´]ê°€ ì œê³µë˜ë©´ ì ê·¹ í™œìš©í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì´ë¦„: ì² ìˆ˜]
```

---

## ğŸ“ TODO 10: ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (í†µí•©!)

### ì „ì²´ êµ¬í˜„ ì½”ë“œ

```python
def generate_response(user_message, username="ì‚¬ìš©ì"):
    """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±"""
    
    print(f"\n{'='*50}")
    print(f"[USER] {username}: {user_message}")
    
    try:
        # ===== 1. ì´ˆê¸° ë©”ì‹œì§€ ì²˜ë¦¬ =====
        if user_message.strip().lower() == "init":
            bot_name = config.get('name', 'ì±—ë´‡')
            
            # ìºë¦­í„°ì— ë§ëŠ” ì¸ì‚¬ë§ ì‘ì„±
            initial_reply = f"ì•ˆë…•! ë‚˜ëŠ” {bot_name}ì´ì•¼. ë­ ê¶ê¸ˆí•œ ê±° ìˆì–´?"
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            try:
                memory.save_context(
                    {"input": ""},
                    {"output": initial_reply}
                )
            except Exception as e:
                print(f"[MEMORY] ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            
            return {
                "reply": initial_reply,
                "image": None
            }
        
        # ===== 2. ë©”ëª¨ë¦¬ ë¡œë“œ =====
        try:
            memory_variables = memory.load_memory_variables({})
            chat_history = memory_variables.get("chat_history", "")
        except Exception as e:
            print(f"[MEMORY] ë¡œë“œ ì˜¤ë¥˜: {e}")
            chat_history = ""
        
        # ===== 3. RAG ê²€ìƒ‰ =====
        context, similarity, metadata = search_similar_documents(
            user_message, 
            collection,
            threshold=0.45,
            top_k=5
        )
        
        has_context = context is not None
        
        # ===== 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± =====
        system_prompt = build_system_prompt(username, has_context)
        
        # ===== 5. ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± =====
        if has_context:
            # RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨
            final_prompt = f"""
{system_prompt}

[ì°¸ê³  ì •ë³´]
{context}

ì‚¬ìš©ì: {user_message}
"""
        else:
            # RAG ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ
            final_prompt = f"""
{system_prompt}

ì‚¬ìš©ì: {user_message}
"""
        
        print(f"[PROMPT] RAG ì‚¬ìš©: {has_context}")
        
        # ===== 6. LLM ì‘ë‹µ ìƒì„± =====
        reply = conversation_chain.predict(
            system_prompt=system_prompt,
            input=user_message
        )
        
        print(f"[BOT] {reply[:100]}...")
        
        # ===== 7. ë©”ëª¨ë¦¬ ì €ì¥ =====
        try:
            memory.save_context(
                {"input": user_message},
                {"output": reply}
            )
        except Exception as e:
            print(f"[MEMORY] ì €ì¥ ì˜¤ë¥˜: {e}")
        
        # ===== 8. ì‘ë‹µ ë°˜í™˜ =====
        return {
            "reply": reply,
            "image": None
        }
        
    except Exception as e:
        print(f"[ERROR] ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            "reply": "ì£„ì†¡í•´ìš”. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "image": None
        }
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. íŒŒì¼ ì§ì ‘ ì‹¤í–‰

```bash
docker-compose exec chatbot python generation/chatbot/chatbot.py
```

### 2. ëŒ€í™” í…ŒìŠ¤íŠ¸

```
ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit): ì•ˆë…•?
[BOT] ì•ˆë…•! ë‚˜ëŠ” ê¹€ì„œê°•ì´ì•¼. ë­ ê¶ê¸ˆí•œ ê±° ìˆì–´?

ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit): í•™ì‹ ì¶”ì²œí•´ì¤˜
[RAG] ê²€ìƒ‰ ì¤‘...
[RAG] ë°œê²¬: í•™ì‹ì€ ê³¤ìê°€ê°€ ë§›ìˆì–´...
[BOT] í•™ì‹ì€ ê³¤ìê°€ì—ì„œ ë¨¹ëŠ” ê²Œ ì¢‹ì•„! ëˆê¹ŒìŠ¤ê°€ íŠ¹íˆ ì¸ê¸°ì•¼.

ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit): quit
```

### 3. ì›¹ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸

```bash
# Docker ì‹¤í–‰
docker-compose up

# ë¸Œë¼ìš°ì €: http://localhost:5000
# ì±„íŒ… í™”ë©´ì—ì„œ í…ŒìŠ¤íŠ¸
```

---

## ğŸ› ë””ë²„ê¹… íŒ

### 1. ë¡œê·¸ ì¶œë ¥ í™œìš©

```python
# ê° ë‹¨ê³„ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
print(f"[DEBUG] ë³€ìˆ˜ í™•ì¸: {variable}")
```

### 2. RAGê°€ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ

```python
# ì„ë² ë”© íŒŒì¼ í™•ì¸
print(f"ë¬¸ì„œ ê°œìˆ˜: {collection.count()}")

# ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
results = collection.query(...)
print(f"ê²€ìƒ‰ ê²°ê³¼: {results}")
```

### 3. ë©”ëª¨ë¦¬ ì˜¤ë¥˜

```python
# verbose=Trueë¡œ ì„¤ì •
conversation_chain = LLMChain(..., verbose=True)
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] TODO 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ
- [ ] TODO 2: ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ
- [ ] TODO 3: OpenAI API í‚¤ ì„¤ì • ì™„ë£Œ
- [ ] TODO 4: ChromaDB ì´ˆê¸°í™” ì™„ë£Œ
- [ ] TODO 5: ì„ë² ë”© ìƒì„± í•¨ìˆ˜ ì™„ë£Œ
- [ ] TODO 6: í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ì™„ë£Œ (ì„ íƒ)
- [ ] TODO 7: RAG ê²€ìƒ‰ í•¨ìˆ˜ ì™„ë£Œ
- [ ] TODO 8: LangChain ì„¤ì • ì™„ë£Œ
- [ ] TODO 9: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ ì™„ë£Œ
- [ ] TODO 10: ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì™„ë£Œ
- [ ] íŒŒì¼ ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì›¹ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“ í•™ìŠµ í¬ì¸íŠ¸

### êµ¬í˜„ì„ í†µí•´ ë°°ìš°ëŠ” ê²ƒ

1. **OpenAI API í™œìš©**
   - ì„ë² ë”© ìƒì„±
   - Chat Completion

2. **RAG êµ¬í˜„**
   - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (ChromaDB)
   - ìœ ì‚¬ë„ ê²€ìƒ‰
   - ì»¨í…ìŠ¤íŠ¸ í™œìš©

3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**
   - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
   - ì»¨í…ìŠ¤íŠ¸ í†µí•©
   - í˜ë¥´ì†Œë‚˜ ì„¤ì •

4. **ë©”ëª¨ë¦¬ ê´€ë¦¬**
   - LangChain Memory
   - ëŒ€í™” ë§¥ë½ ìœ ì§€

5. **í•œêµ­ì–´ NLP**
   - KoNLPy í™œìš©
   - í‚¤ì›Œë“œ ì¶”ì¶œ

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs/api-reference)
- [ChromaDB ë¬¸ì„œ](https://docs.trychroma.com/)
- [LangChain ë¬¸ì„œ](https://python.langchain.com/docs/get_started/introduction)
- [KoNLPy ë¬¸ì„œ](https://konlpy.org/ko/latest/)

### ìœ ìš©í•œ íŠœí† ë¦¬ì–¼
- [RAG íŠœí† ë¦¬ì–¼](https://python.langchain.com/docs/tutorials/rag/)
- [LangChain ë©”ëª¨ë¦¬ ê°€ì´ë“œ](https://python.langchain.com/docs/modules/memory/)

---

**êµ¬í˜„ ì™„ë£Œ í›„ ASSIGNMENT_GUIDE.mdì˜ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”!**

