"""
RAG-D ë°©ì‹ ìƒë‹´ ë§¤ë‰´ì–¼ ë²¡í„° DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” source_pdfs í´ë”ì˜ PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬
ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python tools/build_counseling_vectordb.py
"""

import os
import sys
from pathlib import Path
from typing import List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
PDF_DIR = BASE_DIR / "static" / "data" / "chatbot" / "source_pdfs"
VECTOR_DB_DIR = BASE_DIR / "static" / "data" / "chatbot" / "counseling_vectordb"


def load_pdfs() -> List[Document]:
    """PDF íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    print(f"\nğŸ“š PDF íŒŒì¼ ë¡œë“œ ì‹œì‘...")
    print(f"   ê²½ë¡œ: {PDF_DIR}")
    
    all_documents = []
    pdf_files = list(PDF_DIR.glob("*.pdf"))
    
    if not pdf_files:
        print("   âš ï¸  PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return []
    
    print(f"   ë°œê²¬ëœ PDF: {len(pdf_files)}ê°œ")
    
    for pdf_file in pdf_files:
        try:
            print(f"\n   ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
            loader = PyPDFLoader(str(pdf_file))
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ëª… ì¶”ê°€
            for doc in documents:
                doc.metadata["source_file"] = pdf_file.name
                doc.metadata["type"] = "counseling_manual"
            
            all_documents.extend(documents)
            print(f"      âœ… {len(documents)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"      âŒ ì—ëŸ¬: {e}")
            continue
    
    print(f"\nâœ… ì´ {len(all_documents)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    return all_documents


def split_documents(documents: List[Document]) -> List[Document]:
    """ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• """
    print(f"\nâœ‚ï¸  ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
    
    # ìƒë‹´ ë§¤ë‰´ì–¼ íŠ¹ì„±ì— ë§ëŠ” ì²­í‚¹ ì „ëµ
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # 1000ì ì²­í¬
        chunk_overlap=200,  # 200ì ì¤‘ë³µ (ë§¥ë½ ìœ ì§€)
        length_function=len,
        separators=[
            "\n\n",  # ë‹¨ë½ êµ¬ë¶„ ìš°ì„ 
            "\n",    # ì¤„ë°”ê¿ˆ
            ". ",    # ë¬¸ì¥ ë
            "? ",    # ì§ˆë¬¸ ë
            "! ",    # ê°íƒ„ ë
            " ",     # ê³µë°±
            ""       # ìµœí›„ ìˆ˜ë‹¨
        ],
        is_separator_regex=False,
    )
    
    split_docs = text_splitter.split_documents(documents)
    
    # ì²­í¬ ë©”íƒ€ë°ì´í„° ë³´ê°•
    for i, doc in enumerate(split_docs):
        doc.metadata["chunk_index"] = i
    
    print(f"   âœ… {len(documents)} ë¬¸ì„œ â†’ {len(split_docs)} ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
    print(f"   ğŸ“Š í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(d.page_content) for d in split_docs) // len(split_docs)}ì")
    
    return split_docs


def build_vectordb(chunks: List[Document]):
    """ë²¡í„° DB êµ¬ì¶• (ë°°ì¹˜ ì²˜ë¦¬ë¡œ í† í° ì œí•œ íšŒí”¼)"""
    print(f"\nğŸ”¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘...")
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print(f"   ğŸ§  OpenAI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",  # ë¹ ë¥´ê³  íš¨ìœ¨ì 
        openai_api_key=api_key,
        chunk_size=100  # â­ ë°°ì¹˜ í¬ê¸° ì œí•œ (í† í° ì œí•œ íšŒí”¼)
    )
    
    # ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ (ì¬êµ¬ì¶•)
    if VECTOR_DB_DIR.exists():
        import shutil
        print(f"   ğŸ—‘ï¸  ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì¤‘...")
        shutil.rmtree(VECTOR_DB_DIR)
    
    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)
    
    # ChromaDB êµ¬ì¶• (ë°°ì¹˜ ì²˜ë¦¬)
    print(f"   ğŸ’¾ ChromaDBì— {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
    print(f"   â³ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (OpenAI API í˜¸ì¶œ)...")
    
    try:
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (í† í° ì œí•œ ê³ ë ¤)
        batch_size = 50  # í•œ ë²ˆì— 50ê°œì”© ì²˜ë¦¬
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        # ì²« ë°°ì¹˜ë¡œ ì»¬ë ‰ì…˜ ìƒì„±
        print(f"   ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: ì´ {total_batches}ê°œ ë°°ì¹˜")
        
        vectordb = Chroma.from_documents(
            documents=chunks[:batch_size],
            embedding=embeddings,
            persist_directory=str(VECTOR_DB_DIR),
            collection_name="counseling_knowledge"
        )
        print(f"      âœ… ë°°ì¹˜ 1/{total_batches} ì™„ë£Œ")
        
        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
        for i in range(1, total_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(chunks))
            batch = chunks[start_idx:end_idx]
            
            vectordb.add_documents(batch)
            print(f"      âœ… ë°°ì¹˜ {i+1}/{total_batches} ì™„ë£Œ ({end_idx}/{len(chunks)} ì²­í¬)")
        
        print(f"\nâœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
        print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {VECTOR_DB_DIR}")
        print(f"   ğŸ“¦ ì»¬ë ‰ì…˜ í¬ê¸°: {vectordb._collection.count()} í•­ëª©")
        
        return vectordb
        
    except Exception as e:
        print(f"\nâŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
        raise


def test_search(vectordb):
    """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    
    test_queries = [
        "ìì‚´ ìœ„ê¸° ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?",
        "ë¶ˆì•ˆ ì¦ìƒì„ ê°€ì§„ ë‚´ë‹´ìë¥¼ ì–´ë–»ê²Œ ìƒë‹´í•˜ë‚˜ìš”?",
        "ì •ì‹ ê±´ê°• ìœ„ê¸° ê°œì… ì ˆì°¨ëŠ”?"
    ]
    
    for query in test_queries:
        print(f"\n   ğŸ’¬ ì§ˆë¬¸: {query}")
        results = vectordb.similarity_search(query, k=2)
        
        if results:
            print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
            for i, doc in enumerate(results, 1):
                content_preview = doc.page_content[:150].replace('\n', ' ')
                print(f"      [{i}] {doc.metadata.get('source_file', 'Unknown')}")
                print(f"          {content_preview}...")
        else:
            print(f"   âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸš€ RAG-D ìƒë‹´ ë§¤ë‰´ì–¼ ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
    print("="*60)
    
    try:
        # 1. PDF ë¡œë“œ
        documents = load_pdfs()
        if not documents:
            print("\nâŒ ë¡œë“œí•  PDFê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # 2. ë¬¸ì„œ ë¶„í• 
        chunks = split_documents(documents)
        
        # 3. ë²¡í„° DB êµ¬ì¶•
        vectordb = build_vectordb(chunks)
        
        # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_search(vectordb)
        
        print("\n" + "="*60)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. Docker ì¬ì‹œì‘: docker compose down && docker compose up --build")
        print(f"   2. ì±—ë´‡ì´ ìë™ìœ¼ë¡œ ìƒë‹´ ë§¤ë‰´ì–¼ ì§€ì‹ì„ ì°¸ì¡°í•©ë‹ˆë‹¤")
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

