# ğŸš€ ê³ ê¸‰ ì£¼ì œ ë° ì„±ëŠ¥ ê°œì„  ê°€ì´ë“œ

> í”„ë¡œì íŠ¸ ì™„ì„±ë„ë¥¼ ë†’ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²• (2025ë…„ 10ì›” ê¸°ì¤€)

---

## ğŸ“š ëª©ì°¨

1. [í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ](#-í•µì‹¬-ê¸°ìˆ -ìŠ¤íƒ)
2. [ê´€ë ¨ ë…¼ë¬¸ ë° ì—°êµ¬](#-ê´€ë ¨-ë…¼ë¬¸-ë°-ì—°êµ¬)
3. [ê·¸ ì™¸ í•™ìŠµ ìë£Œ](#-ê·¸ ì™¸-í•™ìŠµ-ìë£Œ)
4. [ì„±ëŠ¥ ê°œì„  ë°©ë²•](#-ì„±ëŠ¥-ê°œì„ -ë°©ë²•)
5. [ì™„ì„±ë„ í–¥ìƒ ì „ëµ](#-ì™„ì„±ë„-í–¥ìƒ-ì „ëµ)
6. [2025ë…„ ìµœì‹  íŠ¸ë Œë“œ](#-2025ë…„-ìµœì‹ -íŠ¸ë Œë“œ)

---

## ğŸ”§ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

### 1. RAG (Retrieval-Augmented Generation)

**ê°œë…**: ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ AI ì‹œìŠ¤í…œ

```
ì§ˆë¬¸ ì…ë ¥
  â†“
ë²¡í„° ê²€ìƒ‰ (ChromaDB)
  â†“
ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
  â†“
LLMì— ë¬¸ì„œ + ì§ˆë¬¸ ì „ë‹¬
  â†“
ì •í™•í•œ ë‹µë³€ ìƒì„±
```

**í•µì‹¬ ë…¼ë¬¸**:

- **"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"** (Lewis et al., 2020)
  - https://arxiv.org/abs/2005.11401
  - RAGì˜ ê¸°ì´ˆë¥¼ ì œì‹œí•œ ë…¼ë¬¸

**ìµœì‹  ë°œì „**:

- **Self-RAG** (2024): ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨
- **Corrective RAG** (C-RAG, 2024): ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€
- **Adaptive RAG** (2024): ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ì „ëµ ë³€ê²½

### 2. Vector Embeddings

**ê°œë…**: í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜

**OpenAI Embeddings API**:

- **text-embedding-3-large**: 3072ì°¨ì›, ê°€ì¥ ì •í™•
- **text-embedding-3-small**: 1536ì°¨ì›, ë¹ ë¥´ê³  ì €ë ´
- **ë¹„ìš©**: $0.13 / 1M tokens (3-large ê¸°ì¤€)

**í•µì‹¬ ë…¼ë¬¸**:

- **"Attention Is All You Need"** (Vaswani et al., 2017)

  - https://arxiv.org/abs/1706.03762
  - Transformer ì•„í‚¤í…ì²˜ì˜ ê¸°ì´ˆ

- **"BERT: Pre-training of Deep Bidirectional Transformers"** (Devlin et al., 2018)
  - https://arxiv.org/abs/1810.04805
  - ë¬¸ë§¥ ê¸°ë°˜ embeddingì˜ í˜ì‹ 

### 3. Vector Database (ChromaDB)

**ê°œë…**: ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰

**ì•Œê³ ë¦¬ì¦˜**: HNSW (Hierarchical Navigable Small World)

- **ì‹œê°„ ë³µì¡ë„**: O(log N)
- **ì •í™•ë„**: 99%+
- **í™•ì¥ì„±**: ìˆ˜ë°±ë§Œ ë²¡í„° ì§€ì›

**ëŒ€ì•ˆ ê¸°ìˆ **:

- **Pinecone**: í´ë¼ìš°ë“œ ê¸°ë°˜, ê´€ë¦¬í˜•
- **Weaviate**: GraphQL ì§€ì›, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- **Milvus**: ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ìš©
- **Qdrant**: Rust ê¸°ë°˜, ê³ ì„±ëŠ¥

### 4. LLM (Large Language Model)

**ì‚¬ìš© ëª¨ë¸**: GPT-4o-mini

- **ë¹„ìš© íš¨ìœ¨ì **: $0.15 / 1M ì…ë ¥ í† í°
- **ë¹ ë¥¸ ì‘ë‹µ**: ~500ms
- **ì»¨í…ìŠ¤íŠ¸**: 128K í† í°

**ìµœì‹  ëŒ€ì•ˆ** (2025ë…„ 10ì›”):
GPT, Claude ë“±

---

## ğŸ“– ê´€ë ¨ ë…¼ë¬¸ ë° ì—°êµ¬

### RAG ê´€ë ¨ í•„ë… ë…¼ë¬¸

1. **"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"**

   - Authors: Lewis et al. (Meta AI)
   - Year: 2020
   - Link: https://arxiv.org/abs/2005.11401
   - **í•µì‹¬**: RAGì˜ ê¸°ë³¸ ê°œë…ê³¼ êµ¬ì¡°

2. **"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"**

   - Authors: Asai et al.
   - Year: 2024
   - Link: https://arxiv.org/abs/2310.11511
   - **í•µì‹¬**: ê²€ìƒ‰ í•„ìš”ì„±ì„ LLMì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨

3. **"Corrective Retrieval Augmented Generation"**

   - Authors: Yan et al.
   - Year: 2024
   - Link: https://arxiv.org/abs/2401.15884
   - **í•µì‹¬**: ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ë³´ì •

4. **"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"**
   - Authors: Sarthi et al.
   - Year: 2024
   - Link: https://arxiv.org/abs/2401.18059
   - **í•µì‹¬**: ê³„ì¸µì  ë¬¸ì„œ êµ¬ì¡°ë¡œ ê²€ìƒ‰ ê°œì„ 

### Embedding ê´€ë ¨

5. **"Text Embeddings by Weakly-Supervised Contrastive Pre-training"**

   - Authors: Wang et al. (OpenAI)
   - Year: 2022
   - Link: https://arxiv.org/abs/2212.03533
   - **í•µì‹¬**: OpenAI embedding ëª¨ë¸ì˜ ê¸°ìˆ ì  ë°°ê²½

6. **"Matryoshka Representation Learning"**
   - Authors: Kusupati et al.
   - Year: 2022
   - Link: https://arxiv.org/abs/2205.13147
   - **í•µì‹¬**: ê°€ë³€ ì°¨ì› embedding (íš¨ìœ¨ì„± í–¥ìƒ)

### ì±—ë´‡ ë° ëŒ€í™” ì‹œìŠ¤í…œ

7. **"Constitutional AI: Harmlessness from AI Feedback"**

   - Authors: Bai et al. (Anthropic)
   - Year: 2022
   - Link: https://arxiv.org/abs/2212.08073
   - **í•µì‹¬**: ì•ˆì „í•˜ê³  ìœ ìš©í•œ ì±—ë´‡ ì„¤ê³„

8. **"LLM-as-a-Judge: Evaluating Chat Assistants with Large Language Models"**
   - Authors: Zheng et al.
   - Year: 2023
   - Link: https://arxiv.org/abs/2306.05685
   - **í•µì‹¬**: ì±—ë´‡ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ë¡ 

---

## ğŸ“š ê·¸ ì™¸ í•™ìŠµ ìë£Œ

### ê³µì‹ ë¬¸ì„œ

1. **OpenAI Documentation**

   - https://platform.openai.com/docs
   - Embeddings, Chat API, Best Practices

2. **LangChain Documentation**

   - https://python.langchain.com/docs
   - RAG êµ¬í˜„ íŒ¨í„´, ë©”ëª¨ë¦¬ ê´€ë¦¬

3. **ChromaDB Documentation**
   - https://docs.trychroma.com/
   - ë²¡í„° DB ìµœì í™” ê°€ì´ë“œ

### ì˜¨ë¼ì¸ ê°•ì˜

4. **DeepLearning.AI - LangChain for LLM Application Development**

   - https://www.deeplearning.ai/short-courses/
   - Andrew Ng êµìˆ˜ ê°•ì˜
   - ë¬´ë£Œ, ê³µì¸ ìˆ˜ë£Œì¦

5. **OpenAI Cookbook**
   - https://cookbook.openai.com/
   - ì‹¤ì „ ì˜ˆì œ ì½”ë“œ ëª¨ìŒ

### ì„œì 

6. **"Building LLM Apps" by Joao Moura (2024)**

   - RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤ì „ ê°€ì´ë“œ

7. **"Generative AI on AWS" by Chris Fregly (2024)**
   - í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

### ë¸”ë¡œê·¸ & ì•„í‹°í´

8. **Pinecone Learning Center**

   - https://www.pinecone.io/learn/
   - Vector DB ê°œë… ì„¤ëª…

9. **LlamaIndex Blog**

   - https://www.llamaindex.ai/blog
   - RAG ìµœì‹  ê¸°ë²•

10. **Anthropic Research**
    - https://www.anthropic.com/research
    - LLM ì•ˆì „ì„± ì—°êµ¬

---

## ğŸš€ ì„±ëŠ¥ ê°œì„  ë°©ë²•

### 1. RAG ì„±ëŠ¥ ìµœì í™”

#### A. Embedding ì „ëµ ê°œì„ 

**í˜„ì¬**:

```python
embedding = openai.embeddings.create(
    input=query,
    model="text-embedding-3-large"
)
```

**ê°œì„  1: ì¿¼ë¦¬ í™•ì¥ (Query Expansion)**

```python
def expand_query(original_query):
    """LLMìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê´€ì ìœ¼ë¡œ í™•ì¥"""
    prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ì„ 3ê°€ì§€ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”:
    ì§ˆë¬¸: {original_query}
    """
    expanded = llm.generate(prompt)
    return [original_query] + expanded

# ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í›„ ê²°ê³¼ ê²°í•©
results = []
for query in expand_query(user_query):
    results.extend(search_similar(query))
```

**ê°œì„  2: Hypothetical Document Embeddings (HyDE)**

```python
def hyde_search(query):
    """ê°€ìƒ ë‹µë³€ì„ ë¨¼ì € ìƒì„± í›„ ê²€ìƒ‰"""
    # 1. LLMìœ¼ë¡œ ê°€ìƒ ë‹µë³€ ìƒì„±
    hypothetical_answer = llm.generate(
        f"ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”: {query}"
    )

    # 2. ê°€ìƒ ë‹µë³€ì˜ embeddingìœ¼ë¡œ ê²€ìƒ‰
    embedding = create_embedding(hypothetical_answer)
    return vector_db.search(embedding)
```

#### B. ë¦¬ë­í‚¹ (Re-ranking)

**ë¬¸ì œ**: ë²¡í„° ê²€ìƒ‰ë§Œìœ¼ë¡œëŠ” ìµœì ì´ ì•„ë‹ ìˆ˜ ìˆìŒ

**í•´ê²°**:

```python
from sentence_transformers import CrossEncoder

# 1ì°¨: ë²¡í„° ê²€ìƒ‰ (ë¹ ë¦„)
candidates = vector_db.search(query, top_k=20)

# 2ì°¨: CrossEncoderë¡œ ì •í™•ë„ ì¬í‰ê°€ (ëŠë¦¼)
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc) for doc in candidates])

# ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
top_docs = sorted(zip(candidates, scores),
                  key=lambda x: x[1], reverse=True)[:5]
```

#### C. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

**ë²¡í„° + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©**:

```python
def hybrid_search(query):
    # ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ë¡ ì )
    vector_results = vector_db.search(query, top_k=10)

    # BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ (ì •í™•í•œ ë§¤ì¹­)
    from rank_bm25 import BM25Okapi
    bm25_results = bm25.get_top_n(query.split(), documents, n=10)

    # ê²°ê³¼ ê²°í•© (RRF - Reciprocal Rank Fusion)
    combined = reciprocal_rank_fusion([vector_results, bm25_results])
    return combined
```

### 2. ì‘ë‹µ ìƒì„± ìµœì í™”

#### A. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

**í˜„ì¬**: ì „ì²´ ì‘ë‹µ ìƒì„± í›„ ë°˜í™˜ (ëŠë¦¼)

**ê°œì„ **:

```python
def generate_response_stream(user_message):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    for chunk in openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[...],
        stream=True  # â† ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    ):
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# Flaskì—ì„œ
@app.route('/api/chat/stream')
def chat_stream():
    def generate():
        for text in generate_response_stream(message):
            yield f"data: {json.dumps({'text': text})}\n\n"
    return Response(generate(), mimetype='text/event-stream')
```

#### B. í”„ë¡¬í”„íŠ¸ ìµœì í™”

**í˜„ì¬**:

```
ë‹¹ì‹ ì€ ì„œê°•ëŒ€ ì„ ë°°ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
```

**ê°œì„ ** (Few-Shot ì˜ˆì‹œ ì¶”ê°€):

```
ë‹¹ì‹ ì€ ì„œê°•ëŒ€ ì„ ë°°ì…ë‹ˆë‹¤.

[ì¢‹ì€ ë‹µë³€ ì˜ˆì‹œ]
ì§ˆë¬¸: í•™ì‹ ì–´ë””ê°€ ë§›ìˆì–´?
ë‹µë³€: í•™ì‹ì€ ê³¤ìê°€ 2ì¸µì´ ì œì¼ ë§›ìˆì–´. íŠ¹íˆ ìˆ˜ìš”ì¼ ëˆê¹ŒìŠ¤ëŠ” ì¤„ ì„œì„œ ë¨¹ì„ ê°€ì¹˜ê°€ ìˆì§€.
      ê°€ê²©ë„ 5,000ì›ìœ¼ë¡œ ì €ë ´í•˜ê³ , ì–‘ë„ í‘¸ì§í•´ì„œ ì¸ê¸°ê°€ ë§ì•„. ì ì‹¬ì‹œê°„(12-1ì‹œ)ì—ëŠ”
      ì‚¬ëŒì´ ë§ìœ¼ë‹ˆ 11ì‹œ 30ë¶„ì— ê°€ëŠ” ê±¸ ì¶”ì²œí•´!

ì´ì œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
```

#### C. ìºì‹±

**ë™ì¼í•œ ì§ˆë¬¸ ë°˜ë³µ ì‹œ ìºì‹±**:

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_search(query_hash):
    """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì€ ìºì‹±"""
    return vector_db.search(query)

# ì‚¬ìš©
query_hash = hash(user_message.lower().strip())
results = cached_search(query_hash)
```

### 3. ë°ì´í„° í’ˆì§ˆ í–¥ìƒ

#### A. ì²­í‚¹ ì „ëµ ê°œì„ 

**í˜„ì¬**: ê³ ì • í¬ê¸° (400ì)

**ê°œì„ ** (ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• ):

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""],  # ìš°ì„ ìˆœìœ„
    length_function=len
)
```

#### B. ë©”íƒ€ë°ì´í„° ì¶”ê°€

```python
# ë” í’ë¶€í•œ ë©”íƒ€ë°ì´í„°
collection.add(
    documents=[text],
    embeddings=[embedding],
    metadatas=[{
        "type": "qa",
        "source": "í•™ì‚¬ê³µì§€",
        "date": "2025-10-01",
        "category": "í•™ì‚¬ì¼ì •",
        "importance": "high",
        "keywords": ["ìˆ˜ê°•ì‹ ì²­", "ì¼ì •"]  # í‚¤ì›Œë“œ íƒœê¹…
    }]
)

# í•„í„°ë§ ê²€ìƒ‰
results = collection.query(
    query_embeddings=[embedding],
    n_results=5,
    where={"category": "í•™ì‚¬ì¼ì •"}  # í•„í„°
)
```

### 4. ë©€í‹°ëª¨ë‹¬ ì§€ì›

#### A. ì´ë¯¸ì§€ ê²€ìƒ‰ ì¶”ê°€

```python
from PIL import Image
import clip  # OpenAI CLIP ëª¨ë¸

def search_similar_image(query_text):
    """í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
    # í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ embedding
    text_embedding = clip.encode_text(query_text)

    # ì´ë¯¸ì§€ DBì—ì„œ ê²€ìƒ‰
    results = image_db.query(text_embedding)
    return results
```

#### B. ìŒì„± ì§€ì›

```python
from openai import OpenAI

# ìŒì„± â†’ í…ìŠ¤íŠ¸
def speech_to_text(audio_file):
    transcript = openai.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file
    )
    return transcript.text

# í…ìŠ¤íŠ¸ â†’ ìŒì„±
def text_to_speech(text):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=text
    )
    return response.content
```

---

## ğŸ’ ì™„ì„±ë„ í–¥ìƒ ì „ëµ

### 1. í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•

#### A. ìë™ í‰ê°€

```python
def evaluate_response(question, answer, ground_truth):
    """LLMìœ¼ë¡œ ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
    eval_prompt = f"""
    ì§ˆë¬¸: {question}
    ë‹µë³€: {answer}
    ì •ë‹µ: {ground_truth}

    ë‹µë³€ì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
    """
    score = llm.generate(eval_prompt)
    return score
```

#### B. ì‚¬ìš©ì í”¼ë“œë°±

```python
# í”¼ë“œë°± ìˆ˜ì§‘
@app.route('/api/feedback', methods=['POST'])
def collect_feedback():
    data = request.json
    # {message_id, rating (1-5), comment}
    save_feedback(data)

    # ë‚®ì€ í‰ê°€ â†’ ìë™ìœ¼ë¡œ ë°ì´í„° ì¶”ê°€
    if data['rating'] <= 2:
        add_to_training_data(data)
```

### 2. ì•ˆì „ì„± ê°•í™”

#### A. ì½˜í…ì¸  í•„í„°ë§

```python
from openai import OpenAI

def moderate_content(text):
    """ìœ í•´ ì½˜í…ì¸  ê°ì§€"""
    response = openai.moderations.create(input=text)
    if response.results[0].flagged:
        return None, "ë¶€ì ì ˆí•œ ë‚´ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    return text, None
```

#### B. PII (ê°œì¸ì •ë³´) ë³´í˜¸

```python
import re

def redact_pii(text):
    """ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹"""
    # ì „í™”ë²ˆí˜¸
    text = re.sub(r'\d{3}-\d{4}-\d{4}', '[ì „í™”ë²ˆí˜¸]', text)
    # ì´ë©”ì¼
    text = re.sub(r'\S+@\S+\.\S+', '[ì´ë©”ì¼]', text)
    # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
    text = re.sub(r'\d{6}-\d{7}', '[ì£¼ë¯¼ë²ˆí˜¸]', text)
    return text
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import time
from functools import wraps

def monitor_performance(func):
    """ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start

        # ë¡œê¹… ë˜ëŠ” ë©”íŠ¸ë¦­ ì „ì†¡
        log_metric('response_time', elapsed)

        if elapsed > 3.0:
            alert('Slow response detected', elapsed)

        return result
    return wrapper
```

### 4. A/B í…ŒìŠ¤íŒ…

```python
def ab_test_strategy(user_id):
    """ë‹¤ì–‘í•œ RAG ì „ëµ í…ŒìŠ¤íŠ¸"""
    strategy = hash(user_id) % 3

    if strategy == 0:
        # ê¸°ë³¸ RAG
        return basic_rag
    elif strategy == 1:
        # HyDE ì „ëµ
        return hyde_rag
    else:
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        return hybrid_rag
```

---

## ğŸŒŸ 2025ë…„ ìµœì‹  íŠ¸ë Œë“œ

### 1. Agentic RAG

**ê°œë…**: LLMì´ ìŠ¤ìŠ¤ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰

```python
from langchain.agents import create_react_agent

tools = [
    VectorSearchTool(),
    WebSearchTool(),
    CalculatorTool(),
    WikipediaTool()
]

agent = create_react_agent(
    llm=ChatOpenAI(model="gpt-4o"),
    tools=tools,
    prompt=agent_prompt
)

# LLMì´ í•„ìš”ì— ë”°ë¼ ë„êµ¬ ì„ íƒ
result = agent.run("2024ë…„ ë…¸ë²¨ë¬¼ë¦¬í•™ìƒ ìˆ˜ìƒìëŠ”?")
```

### 2. Graph RAG

**ê°œë…**: ì§€ì‹ì„ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ì €ì¥

```python
from neo4j import GraphDatabase

# ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•
CREATE (í•™ì‹:Entity {name: "í•™ì‹"})
CREATE (ê³¤ìê°€:Entity {name: "ê³¤ìê°€"})
CREATE (í•™ì‹)-[:LOCATED_AT]->(ê³¤ìê°€)
CREATE (í•™ì‹)-[:HAS_MENU {price: 5000}]->(ëˆê¹ŒìŠ¤)

# ê·¸ë˜í”„ ê¸°ë°˜ ê²€ìƒ‰
def graph_rag(query):
    # 1. ì—”í‹°í‹° ì¶”ì¶œ
    entities = extract_entities(query)

    # 2. ê·¸ë˜í”„ íƒìƒ‰
    subgraph = neo4j.query(f"MATCH path=({entities[0]})-[*1..3]-()")

    # 3. ì„œë¸Œê·¸ë˜í”„ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
    return generate_with_graph_context(query, subgraph)
```

### 3. ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

```python
# ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ë“¤
researcher_agent = Agent(role="ì •ë³´ ìˆ˜ì§‘", tools=[search])
writer_agent = Agent(role="ë‹µë³€ ì‘ì„±", tools=[llm])
critic_agent = Agent(role="í’ˆì§ˆ ê²€ì¦", tools=[evaluator])

def multi_agent_response(query):
    # 1. ì •ë³´ ìˆ˜ì§‘
    info = researcher_agent.run(query)

    # 2. ë‹µë³€ ì‘ì„±
    draft = writer_agent.run(info)

    # 3. ê²€ì¦ ë° ê°œì„ 
    final = critic_agent.review_and_improve(draft)

    return final
```

### 4. ì§€ì†ì  í•™ìŠµ (Continual Learning)

```python
def continual_learning():
    """ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ì§€ì† ê°œì„ """

    # 1. í”¼ë“œë°± ìˆ˜ì§‘
    feedbacks = get_new_feedbacks()

    # 2. íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ êµ¬ì¶•
    training_data = []
    for fb in feedbacks:
        if fb.rating >= 4:  # ì¢‹ì€ ë‹µë³€
            training_data.append({
                "question": fb.question,
                "answer": fb.answer
            })

    # 3. Embedding ì¬í•™ìŠµ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ê°œì„ 
    update_system_prompt(training_data)

    # 4. ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
    add_to_vector_db(training_data)
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ëª©í‘œ ì§€í‘œ (í”„ë¡œë•ì…˜)

| ë©”íŠ¸ë¦­        | ëª©í‘œ         | ì¸¡ì • ë°©ë²•   |
| ------------- | ------------ | ----------- |
| **ì‘ë‹µ ì‹œê°„** | < 2ì´ˆ        | í‰ê·         |
| **ì •í™•ë„**    | > 85%        | ì‚¬ìš©ì í‰ê°€ |
| **ë§Œì¡±ë„**    | > 4.0/5.0    | í”¼ë“œë°±      |
| **ê°€ìš©ì„±**    | > 99%        | Uptime      |
| **ë¹„ìš©**      | < $0.05/ëŒ€í™” | API ë¹„ìš©    |

### ì¸¡ì • ë„êµ¬

```python
import prometheus_client as prom

# ë©”íŠ¸ë¦­ ì •ì˜
response_time = prom.Histogram('response_time_seconds', 'Response time')
accuracy_score = prom.Gauge('accuracy_score', 'Answer accuracy')

# ì¸¡ì •
with response_time.time():
    answer = generate_response(query)

accuracy_score.set(evaluate_accuracy(answer))
```

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì™„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ (í•„ìˆ˜)

- [ ] RAG ì‹œìŠ¤í…œ ë™ì‘
- [ ] 5ê°œ ì´ìƒ Q&A ë°ì´í„°
- [ ] ê¸°ë³¸ UI ì™„ì„±
- [ ] Vercel ë°°í¬

### ì¤‘ê¸‰ (ê¶Œì¥)

- [ ] 20ê°œ ì´ìƒ Q&A ë°ì´í„°
- [ ] ë¦¬ë­í‚¹ êµ¬í˜„
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- [ ] ì—ëŸ¬ ì²˜ë¦¬
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

### ê³ ê¸‰ (ì°¨ë³„í™”)

- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- [ ] ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€)
- [ ] A/B í…ŒìŠ¤íŒ…
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ì§€ì†ì  í•™ìŠµ

---

## ğŸ“– ë” ì½ì–´ë³¼ ìë£Œ

### ìµœì‹  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ (2025)

1. **"State of RAG 2025"** - LangChain Blog
   - https://blog.langchain.com/
2. **"Production RAG at Scale"** - Pinecone

   - https://www.pinecone.io/blog/

3. **"Advanced RAG Techniques"** - LlamaIndex
   - https://www.llamaindex.ai/blog/
