"""
âœï¸ ì±—ë´‡ ë¡œì§ êµ¬í˜„ íŒŒì¼ (í•™ìƒì´ ì§ì ‘ ì‘ì„±)

ì´ íŒŒì¼ì€ í•™ìƒì´ ì²˜ìŒë¶€í„° ì§ì ‘ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
ì•„ë˜ëŠ” ê¸°ë³¸ êµ¬ì¡°ì™€ ê°€ì´ë“œë§Œ ì œê³µë©ë‹ˆë‹¤.

ğŸ“ êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥:
  1. OpenAI API ì—°ë™
  2. ChromaDBë¥¼ ì´ìš©í•œ ì„ë² ë”© ì €ì¥/ê²€ìƒ‰ (RAG)
  3. LangChainì„ ì´ìš©í•œ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
  4. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
  5. í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (KoNLPy)
  6. ì‘ë‹µ ìƒì„± ë¡œì§

ğŸ’¡ ì°¸ê³ :
  - ASSIGNMENT_GUIDE.mdì— ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œ ìˆìŒ
  - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” requirements.txtì— ì´ë¯¸ í¬í•¨ë¨
  - OpenAI API í‚¤ëŠ” .env íŒŒì¼ì—ì„œ ë¡œë“œ
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(BASE_DIR))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=BASE_DIR / ".env")

# ============================================================================
# TODO 1: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================
# íŒíŠ¸:
# - OpenAI API: from openai import OpenAI
# - ChromaDB: import chromadb
# - LangChain: from langchain_community.chat_models import ChatOpenAI
#             from langchain.memory import ConversationSummaryBufferMemory
#             from langchain.chains import LLMChain
#             from langchain.prompts import PromptTemplate
# - KoNLPy: from konlpy.tag import Okt
# - ê¸°íƒ€: uuid, warnings ë“±

# ì—¬ê¸°ì— ì„í¬íŠ¸ ì‘ì„±


# ============================================================================
# TODO 2: ì„¤ì • íŒŒì¼ ë¡œë“œ
# ============================================================================
# config/chatbot_config.json íŒŒì¼ì„ ì½ì–´ì„œ ì±—ë´‡ ì„¤ì • ë¡œë“œ
# íŒíŠ¸: json.load() ì‚¬ìš©

CONFIG_PATH = BASE_DIR / 'config' / 'chatbot_config.json'

# ì—¬ê¸°ì— ì„¤ì • ë¡œë“œ ì½”ë“œ ì‘ì„±
config = {}


# ============================================================================
# TODO 3: OpenAI API í‚¤ ì„¤ì •
# ============================================================================
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì½ì–´ì„œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# íŒíŠ¸: os.getenv("OPENAI_API_KEY")

api_key = None  # ì—¬ê¸°ì— API í‚¤ ë¡œë“œ
# client = OpenAI(api_key=api_key)


# ============================================================================
# TODO 4: ChromaDB ì´ˆê¸°í™” í•¨ìˆ˜ êµ¬í˜„
# ============================================================================
# í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì €ì¥í•  ChromaDB ì´ˆê¸°í™”
# 
# íŒíŠ¸:
# 1. chromadb.PersistentClient()ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
# 2. get_or_create_collection()ë¡œ ì»¬ë ‰ì…˜ ìƒì„±
# 3. ê²½ë¡œ: BASE_DIR / "static" / "data" / "chatbot" / "chardb_embedding"

def init_text_db():
    """
    í…ìŠ¤íŠ¸ ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    
    Returns:
        tuple: (dbclient, collection)
    
    êµ¬í˜„ ë‹¨ê³„:
    1. ì €ì¥ ê²½ë¡œ ì„¤ì • (static/data/chatbot/chardb_embedding)
    2. ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    3. PersistentClient ìƒì„±
    4. ì»¬ë ‰ì…˜ ìƒì„± (ì´ë¦„: "rag_collection")
    5. (dbclient, collection) ë°˜í™˜
    """
    # ì—¬ê¸°ì— êµ¬í˜„
    pass


# ì „ì—­ ë³€ìˆ˜ë¡œ DB ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
# text_dbclient, collection = init_text_db()


# ============================================================================
# TODO 5: ì„ë² ë”© ìƒì„± í•¨ìˆ˜ êµ¬í˜„
# ============================================================================
# OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
#
# íŒíŠ¸:
# client.embeddings.create(input=[text], model="text-embedding-3-large")

def get_embedding(text, model="text-embedding-3-large"):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    
    Args:
        text (str): ë³€í™˜í•  í…ìŠ¤íŠ¸
        model (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸
    
    Returns:
        list: ì„ë² ë”© ë²¡í„°
    
    êµ¬í˜„ ë‹¨ê³„:
    1. OpenAI APIì˜ embeddings.create() í˜¸ì¶œ
    2. response.data[0].embedding ë°˜í™˜
    """
    # ì—¬ê¸°ì— êµ¬í˜„
    pass


# ============================================================================
# TODO 6: í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ êµ¬í˜„ (ì„ íƒ)
# ============================================================================
# KoNLPyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ (RAG ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
#
# íŒíŠ¸:
# - Okt().nouns(text)ë¡œ ëª…ì‚¬ ì¶”ì¶œ
# - ë¶ˆìš©ì–´ ì œê±° (ì˜ˆ: "ê²ƒ", "ìˆ˜", "ë“±")
# - 2ê¸€ì ì´ìƒë§Œ ì„ íƒ

# okt = Okt()

def extract_nouns_korean(text):
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        text (str): ì…ë ¥ í…ìŠ¤íŠ¸
    
    Returns:
        list: ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    êµ¬í˜„ ë‹¨ê³„:
    1. Okt().nouns()ë¡œ ëª…ì‚¬ ì¶”ì¶œ
    2. ë¶ˆìš©ì–´ í•„í„°ë§
    3. 2ê¸€ì ì´ìƒë§Œ ì„ íƒ
    """
    # ì—¬ê¸°ì— êµ¬í˜„ (ì„ íƒ ì‚¬í•­)
    return []


# ============================================================================
# TODO 7: RAG ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜ êµ¬í˜„
# ============================================================================
# ChromaDBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•µì‹¬ RAG ë¡œì§
#
# íŒíŠ¸:
# 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
# 2. collection.query()ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
# 3. ìœ ì‚¬ë„ ê³„ì‚°: similarity = 1 / (1 + distance)
# 4. threshold ì´ìƒì¸ ë¬¸ì„œë§Œ ë°˜í™˜

def search_similar_documents(query, collection, threshold=0.45, top_k=5):
    """
    ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (RAGì˜ í•µì‹¬)
    
    Args:
        query (str): ê²€ìƒ‰ ì§ˆì˜
        collection: ChromaDB ì»¬ë ‰ì…˜
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.3~0.5 ê¶Œì¥)
        top_k (int): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
    
    Returns:
        tuple: (best_document, similarity, metadata) ë˜ëŠ” (None, None, None)
    
    êµ¬í˜„ ë‹¨ê³„:
    1. ì¿¼ë¦¬ì˜ ì„ë² ë”© ìƒì„±
    2. collection.query()ë¡œ ê²€ìƒ‰
    3. ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜: 1 / (1 + distance)
    4. threshold ì´ìƒì¸ ë¬¸ì„œ í•„í„°ë§
    5. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ë°˜í™˜
    
    ğŸ’¡ íŒ:
    - extract_nouns_korean()ì„ ì‚¬ìš©í•˜ë©´ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
    - ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ë©´ ë””ë²„ê¹…ì— ë„ì›€ë¨
    """
    try:
        # ì—¬ê¸°ì— êµ¬í˜„
        
        # 1. ì„ë² ë”© ìƒì„±
        # query_embedding = get_embedding(query)
        
        # 2. ê²€ìƒ‰
        # results = collection.query(
        #     query_embeddings=[query_embedding],
        #     n_results=top_k,
        #     include=["documents", "distances", "metadatas"]
        # )
        
        # 3. ìœ ì‚¬ë„ ê³„ì‚° ë° í•„í„°ë§
        # for doc, dist, meta in zip(documents, distances, metadatas):
        #     similarity = 1 / (1 + dist)
        #     if similarity >= threshold:
        #         ...
        
        # 4. ìµœê³  ìœ ì‚¬ë„ ë¬¸ì„œ ë°˜í™˜
        
        pass
    except Exception as e:
        print(f"[RAG] ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None, None, None


# ============================================================================
# TODO 8: LangChain ë©”ëª¨ë¦¬ ë° ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”
# ============================================================================
# LangChainì„ ì‚¬ìš©í•œ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬
#
# íŒíŠ¸:
# 1. ChatOpenAIë¡œ LLM ì´ˆê¸°í™” (model="gpt-4o-mini", temperature=0.7)
# 2. ConversationSummaryBufferMemoryë¡œ ë©”ëª¨ë¦¬ ìƒì„±
# 3. PromptTemplateìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì •ì˜
# 4. LLMChainìœ¼ë¡œ ëŒ€í™” ì²´ì¸ ìƒì„±

# ì—¬ê¸°ì— LangChain ì„¤ì • ì½”ë“œ ì‘ì„±
# langchain_llm = ChatOpenAI(...)
# memory = ConversationSummaryBufferMemory(...)
# prompt = PromptTemplate(...)
# conversation_chain = LLMChain(...)


# ============================================================================
# TODO 9: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ êµ¬í˜„
# ============================================================================
# config íŒŒì¼ì˜ ì„¤ì •ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±

def build_system_prompt(username, has_context=False):
    """
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        username (str): ì‚¬ìš©ì ì´ë¦„
        has_context (bool): RAG ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        str: ì™„ì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    
    êµ¬í˜„ ë‹¨ê³„:
    1. configì—ì„œ system_prompt ê°€ì ¸ì˜¤ê¸°
    2. ìºë¦­í„° ì •ë³´ ì¶”ê°€
    3. ëŒ€í™” ê·œì¹™ ì¶”ê°€
    4. RAG ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­ ì¶”ê°€
    
    ì˜ˆì‹œ:
    ```
    ë‹¹ì‹ ì€ {ìºë¦­í„° ì´ë¦„}ì…ë‹ˆë‹¤.
    
    ìºë¦­í„° ì •ë³´:
    - ë‚˜ì´: 20ì„¸
    - ëŒ€í•™: ì„œê°•ëŒ€í•™êµ
    - ì „ê³µ: ì»´í“¨í„°ê³µí•™
    
    ëŒ€í™” ê·œì¹™:
    - ë°˜ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
    - ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”
    
    {RAG ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì‹œ: ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”}
    
    [ì‚¬ìš©ì ì´ë¦„: {username}]
    ```
    """
    # ì—¬ê¸°ì— êµ¬í˜„
    pass


# ============================================================================
# TODO 10: ì‘ë‹µ ìƒì„± í•¨ìˆ˜ êµ¬í˜„ (í•µì‹¬!)
# ============================================================================
# ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜

def generate_response(user_message, username="ì‚¬ìš©ì"):
    """
    ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µ ìƒì„±
    
    Args:
        user_message (str): ì‚¬ìš©ì ì…ë ¥
        username (str): ì‚¬ìš©ì ì´ë¦„
    
    Returns:
        dict: {'reply': str, 'image': str or None}
    
    êµ¬í˜„ ë‹¨ê³„:
    
    1. ì´ˆê¸° ë©”ì‹œì§€ ì²˜ë¦¬
       - user_message == "init"ì´ë©´ ì¸ì‚¬ë§ ë°˜í™˜
       - configì—ì„œ ì±—ë´‡ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
       - ë©”ëª¨ë¦¬ì— ì €ì¥
    
    2. RAG ê²€ìƒ‰
       - search_similar_documents()ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
       - ì°¾ìœ¼ë©´ has_context=True
    
    3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
       - build_system_prompt() í˜¸ì¶œ
       - RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ ê²°ì •
    
    4. LLM ì‘ë‹µ ìƒì„±
       - conversation_chain.predict() í˜¸ì¶œ
       - RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    
    5. ë©”ëª¨ë¦¬ ì €ì¥
       - memory.save_context()ë¡œ ëŒ€í™” ì €ì¥
    
    6. ì‘ë‹µ ë°˜í™˜
       - {'reply': ì‘ë‹µí…ìŠ¤íŠ¸, 'image': None}
    
    ğŸ’¡ íŒ:
    - ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ë©´ ë””ë²„ê¹…ì— í° ë„ì›€ë¨
    - RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— ëª…í™•íˆ í‘œì‹œ
    - ì—ëŸ¬ ì²˜ë¦¬ í•„ìˆ˜
    
    ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
    ```
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {build_system_prompt()}
    
    [ì°¸ê³  ì •ë³´] (RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ)
    {ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©}
    
    ëŒ€í™” ê¸°ë¡: {memory}
    
    ì‚¬ìš©ì: {user_message}
    
    ì±—ë´‡:
    ```
    """
    
    print(f"\n{'='*50}")
    print(f"[USER] {username}: {user_message}")
    
    # ì—¬ê¸°ì— ì „ì²´ ë¡œì§ êµ¬í˜„
    
    try:
        # 1. ì´ˆê¸° ë©”ì‹œì§€ ì²˜ë¦¬
        if user_message.strip().lower() == "init":
            # ì´ˆê¸° ì¸ì‚¬ë§ ìƒì„± ë° ë°˜í™˜
            pass
        
        # 2. RAG ê²€ìƒ‰
        # context, similarity, metadata = search_similar_documents(...)
        
        # 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        # system_prompt = build_system_prompt(...)
        
        # 4. LLM ì‘ë‹µ ìƒì„±
        # reply = conversation_chain.predict(...)
        
        # 5. ë©”ëª¨ë¦¬ ì €ì¥
        # memory.save_context(...)
        
        # 6. ì‘ë‹µ ë°˜í™˜
        return {
            "reply": "êµ¬í˜„ í•„ìš”: generate_response() í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”",
            "image": None
        }
        
    except Exception as e:
        print(f"[ERROR] ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
        return {
            "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "image": None
        }


# ============================================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ (ì„ íƒ)
# ============================================================================
if __name__ == "__main__":
    """
    ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
    
    ì‚¬ìš©ë²•:
    docker-compose exec chatbot python generation/chatbot/chatbot.py
    """
    print("=" * 50)
    print("ì±—ë´‡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 50)
    
    # ê°„ë‹¨í•œ ëŒ€í™” í…ŒìŠ¤íŠ¸
    while True:
        user_input = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit): ")
        if user_input.lower() == 'quit':
            break
        
        response = generate_response(user_input, "í…ŒìŠ¤í„°")
        print(f"\n[BOT] {response['reply']}")
    
    print("\ní…ŒìŠ¤íŠ¸ ì¢…ë£Œ")